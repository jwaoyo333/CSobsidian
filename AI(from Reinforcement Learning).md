#### 지능 에이전트
- 인간의 지능적인 일을 대신해주는 에이전트
- 지능 에이전트의 라이프 사이클
	1. 문제 정의
		1. 비즈니스 정의
		2. 데이터 수집
	2. 모델(에이전트) 제작
		1. 모델 설계
		2. 모델 학습
		3. 모델 평가
	3. 디프로이(배포)
		1. 지식 베이스 부착
		2. UI 부착
		3. 현장 평가
- 지식 표현과 추론
	- 두 가지는 인간 지능의 핵심. But 인공지능은 아직 부족한 부분
	- 방법론
		1. 규칙 기반 : if-then 구절을 연속적으로 덧붙여 지식을 표현. 1980년대 주요기술
		2. 프레임 : 슬롯 - 값쌍으로 지식 표현. like DB e.g KL-ONE 패키지
		3. 의미망 : 그래프로 지식을 표현, is-a/kind-of 관계 표현에 유리

#### 강화학습

- 인간의 학습 과정을 모방한 learning method
- 대표적 방법론
	- **탐험형**과 **탐사형**
		1. 탐험형 : 무작위로 goal을 달성하고자 하는 전략
		2. 탐사형 :  현재의 확률을 기반으로 goal을 달성하고자 하는 전략
		3. 탐험형만 계속하면 효율이 떨어지고, 탐사형만 계속하면 더 좋은 길이나 새로운 경로를 발견하지 못함 -> 둘 사이 균형이 중요한 이유
		4. 둘 사이 균형 전략의 대표적 예시 : 엡실론 탐욕 알고리즘, 탐사형의 대표적 사례인 탐욕 알고리즘에 엡실론 비율 만큼의 탐험형 정책을 추가한 느낌.
	- 몬테카를로 방법 : 수학적 난수를 생성하여 확률 문제의 해를 구하는 방법(큰 수의 법칙)
	- 계산 모형
		- 마르코프 결정 프로세스
			-상태, 행동, 보상으로 상태 변환을 규정. 세 가지 요소를 활용하여 문제를 해결하는 전략
		- 상태 전이 방식
			-결정론적 환경 : 100% 확률로 액션이 상태 변화를 일으키는 방식
			-스토캐스틱 환경 : 확률 분포에 따라 액션이 변화를 일으키는지 마는지가 정해지는 환경
			//본 강의는 결정론적 환경만을 다룸//
- 지도학습과의 비교
	- 데이터
		- 강화 학습 : 환경(상태 전이 확률 분포) / 환경에서 수집한 에피소드 데이터
		- 지도 학습 : 훈련 집합 X(특징 벡터)와 Y(레이블)
	- 최적화 목표
		- 강화 학습 : 누적 보상 최대화
		- 지도 학습 : ||x-y|| 최소화
	- 학습 알고리즘이 알아내야 하는 것
		- 강화 학습 : 누적 보상을 최대화 하는 **최적정책
		- 지도 학습 : 오차를 최소화하는 **신경망의 가중치**
	- 품질 평가 함수
		- 강화 학습 : 가치 함수
		- 지도 학습 : 손실 함수
	- 학습 알고리즘
		- 강화 학습 :  프로그래밍 , Sarsa, Q러닝, DQN 등
		- 지도 학습 : 스토케스틱 그레이디언트 하강법 SGD동적
- 최적 정책
	- 강화학습의 목표 = 누적 보상의 최대화.
	- 이때, 모델의 행동을 규정하는 것이 정책이고. 이 정책들 중 누적 보상이 최대가 되는 것을 최적 정책이라 말함
- 가치 함수
	- 정책의 품질을 평가하는 함수
	- 가치 함수가 V(s)일 때, argmaxV(s)가 되게 하는 정책 파이가 최적 정책이라 말할 수 있음
	- **벨만 기대 방정식**
		+ 가치 함수를 계산하기 위한 방정식
		+ 상태가 무한대에 가까운 문제에서 사용
		+ 상태는 서로 밀접한 연관을 갖는 것에서 착안
		+ 식은 우변에 자기 자신을 갖는 순환식의 형태임
		  ![[Pasted image 20240616140310.png]]
		- 이때, (r+v(s')) 부분(s에서 a를 했을 경우의 보상과 다음 상태의 가치)을 Q-function, 행동 가치 함수라고 함
	- **할인율
		- 강화학습을 할 때는 보상이 언제 들어오느냐도 중요한 요소임
		- 때문에, 지연되는 보상에는 할인율 감마를 적용하여 가치를 일정비율로 삭감함
	- 최종 함수
		![[Pasted image 20240616140544.png]]
- 알고리즘
	- 가치 함수를 이용하여 정책의 가치를 평가하는 과정
	- 동적 프로그래밍 기반 알고리즘
		- cf. 동적 프로그래밍이란? 문제를 가장 작은 단위까지 분해한 뒤, 가장 작은 문제부터 해결하는 상향식 문제 해결론 e.g Divided and conquer
		- 정책 반복 알고리즘
			- ![[Pasted image 20240616140755.png]]
		- 가치 반복 알고리즘
			- ![[Pasted image 20240616140812.png]]
		- 동적 프로그래밍은 모두 부트스트랩 방식임.
			- cf. 부트스트램(random sampling with replacement) : 부정확한 값에서 출발 이웃 상태와 정보를 주고받으며 점점 수렴해가는 방식
		- ***동적 프로그래밍의 한계
			- 마르코프 결정 프로세스를 알아야만 적용 가능함
			- 또한, 표를 사용하기 때문에 크기가 작은 문제에만 적용 가능함
	- 학습 기반 알고리즘
		- 상태 전이 확률을 알아야만 가능한 동적 프로그래밍과 달리, 훈련 데이터를 활용한 방식
		- 환경을 시뮬레이션하여 에피소드를 수집
		- 해당 에피소드의 데이터를 토대로 가치 함수를 계산 : 몬테카를로 방법
			- ![[Pasted image 20240616141132.png]]
			- But, 몬테카를로 방법은 데이터를 활용한 학습 기반은 맞지만, 이웃 정보를 고려하지 않기에 부트스트랩 방식은 아님.
			- then, 두 가지 장점(몬테데이터가 필요없다/부트스트랩overfitting이 적다)을 결합한 방식은? -> 시간차 학습 알고리즘
	- 시간차 학습 알고리즘
		- 몬테카를로 방식은 모든 샘플이 종료 순간까지 도달해야 확률 계산이 가능함
		- 시간차 학습은 종료 순간까지 가보지 않고 이웃 상태와의 비교를 통해 가치함수를 개선함
		- 시간차 학습은 에피소드를 활용하기 때문에 학습 기반이고, 이웃 상태와 비교하기에 부트스트랩 방식임
		- Sarsa
			- ![[Pasted image 20240616141434.png]]
			- on policy 방식(다음 행동을 현재 정책을 기반으로 계산함) : 행동 정책 == 타켓 정책
		- Q러닝
			- ![[Pasted image 20240616141530.png]]
			- 꺼진 정책 (다음 행동이 현재 정책이 아닌 max 연산자를 통해 결정됨)
		- ![[Pasted image 20240616141617.png]]
		- ![[Pasted image 20240616142218.png]]
	- 딥러닝
		- 참조표 방식의 한계를 극복 가능
		- 신경망을 사용(신경망 역시 일종의 함수 이므로 가치 함수 역할 가능), 강화 학습을 진행함
		- 핵심은 학습에 사용할 훈련 집합 수집법을 찾는 법 : '특징 벡터-레이블' 샘플 수집이 가능한지, 레이블을 어떻게 달지
		- DQN
			- 딥러닝과 Q러닝을 결합하여 뛰어난 성능을 달성하는 모델
			- 신경망이 가치함수의 역할을 대신
			- Q러닝에서 생성되는 에피소드로부터 샘플 수집
			- 에피소드 t 순간에서 St, Rt->at->St+1,Rt+1이라 하면
			- ![[Pasted image 20240616142620.png]]
			- ![[Pasted image 20240616142645.png]]
- 강화학습과 강한 인공지능
	- 강화 학습은 강한 인공지능에 한 발 더 가까운 상태
		- 비지도 학습
		- 자율 플레이로 학습
		- 같은 신경망 구조로 다른 게임 적용 가능(과업 간 일반화 능력)

